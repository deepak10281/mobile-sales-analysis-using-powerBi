{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepak10281/mobile-sales-analysis-using-powerBi/blob/main/NBs/xinfinity_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8dcdd7c",
      "metadata": {
        "id": "c8dcdd7c"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba162db2",
      "metadata": {
        "id": "ba162db2",
        "outputId": "0fd05364-3509-4e64-8be9-be6cb4bb144d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas\n",
        "import os\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from uuid import uuid4\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(r'C:\\Users\\VR679RX\\OneDrive - EY\\Documents\\Projects\\Projects - 2024\\LangGraph_Academy\\gpt4o.env')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb6fce1",
      "metadata": {
        "id": "4fb6fce1"
      },
      "outputs": [],
      "source": [
        "db = SQLDatabase.from_uri(\"sqlite:///xinfinity_agent.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5bee7e",
      "metadata": {
        "id": "de5bee7e",
        "outputId": "07dba90c-61ae-4a6c-e0d3-e4a2b1f0fedc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sqlite\n",
            "['billing_table', 'customer_service_table', 'customer_table']\n"
          ]
        }
      ],
      "source": [
        "# Print the dialect and available tables\n",
        "print(db.dialect)\n",
        "print(db.get_usable_table_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09231b1c",
      "metadata": {
        "id": "09231b1c"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    query: str\n",
        "    result: str\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56408a3a",
      "metadata": {
        "id": "56408a3a"
      },
      "outputs": [],
      "source": [
        "llm = AzureChatOpenAI(temperature=0,\n",
        "                           api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "                           azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
        "                           openai_api_version=os.getenv('AZURE_OPENAI_VERSION'),\n",
        "                           azure_deployment=os.getenv('AZURE_GPT4O_MODEL')\n",
        "                           )\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "                            api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "                            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
        "                            azure_deployment=os.getenv('AZURE_OPENAI_EMBEDDINGS_MODEL'),\n",
        "                            openai_api_version=os.getenv('AZURE_OPENAI_VERSION'),\n",
        "                            chunk_size = 1\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0efe18a4",
      "metadata": {
        "id": "0efe18a4"
      },
      "source": [
        "### ReAct SQL Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "450b822d",
      "metadata": {
        "id": "450b822d",
        "outputId": "50a26def-6411-4aa4-892e-6b76a20bc2da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x0000021271FA3A10>),\n",
              " InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x0000021271FA3A10>),\n",
              " ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x0000021271FA3A10>),\n",
              " QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x0000021271FA3A10>, llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002127ED467E0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000021271FE13A0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x0000021275927440>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000002127ED46900>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), azure_endpoint='https://train-openai-ey.openai.azure.com/', deployment_name='gpt-4o', openai_api_version='2024-08-01-preview', openai_api_type='azure'), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002127ED467E0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000021271FE13A0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x0000021275927440>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000002127ED46900>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), azure_endpoint='https://train-openai-ey.openai.azure.com/', deployment_name='gpt-4o', openai_api_version='2024-08-01-preview', openai_api_type='azure'), output_parser=StrOutputParser(), llm_kwargs={}))]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "\n",
        "tools = toolkit.get_tools()\n",
        "\n",
        "tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f482beb",
      "metadata": {
        "id": "2f482beb",
        "outputId": "e52d75df-9aaf-43e5-907d-7a9583aac7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m System Message \u001b[0m================================\n",
            "\n",
            "You are an intelligent assistant helping users explore a customer billing system.\n",
            "Given an input question, create a syntactically correct \u001b[33;1m\u001b[1;3m{dialect}\u001b[0m query to run, then look at the results of the query and return the answer.\n",
            "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most \u001b[33;1m\u001b[1;3m{top_k}\u001b[0m results.\n",
            "You can order the results by a relevant column to return the most interesting examples in the database.\n",
            "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
            "You have access to tools for interacting with the database.\n",
            "Only use the below tools. Only use the information returned by the below tools to construct your final answer.\n",
            "You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n",
            "\n",
            "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
            "\n",
            "To start you should ALWAYS look at the tables in the database to see what you can query.\n",
            "Do NOT skip this step.\n",
            "Then you should query the schema of the most relevant tables. \n",
            "Before executing a query, make sure you have:\n",
            "1. The correct **table** to query.\n",
            "2. All **mandatory fields** for that table.\n",
            "3. Clear disambiguation if needed (e.g., same name, multiple services).\n",
            "\n",
            "It is important that You ask follow-up questions to gather missing fields based on this schema:\n",
            "- `customer_table` needs customer_id.\n",
            "- `customer_service_table` needs customer_id + service_id.\n",
            "- `billing_table` needs customer_id + service_id (+ optional billing_date, status, plan_name).\n",
            "\n",
            "When ready, construct a SQL-like query or structured representation to fetch the data.\n"
          ]
        }
      ],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt_template = hub.pull(\"langchain-ai/sql-agent-system-prompt\")\n",
        "\n",
        "assert len(prompt_template.messages) == 1\n",
        "prompt_template.messages[0].prompt.template = \"\"\"You are an intelligent assistant helping users explore a customer billing system.\\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\\nYou can order the results by a relevant column to return the most interesting examples in the database.\\nNever query for all the columns from a specific table, only ask for the relevant columns given the question.\\nYou have access to tools for interacting with the database.\\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\\n\\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\\n\\nTo start you should ALWAYS look at the tables in the database to see what you can query.\\nDo NOT skip this step.\\nThen you should query the schema of the most relevant tables. \\nBefore executing a query, make sure you have:\n",
        "1. The correct **table** to query.\n",
        "2. All **mandatory fields** for that table.\n",
        "3. Clear disambiguation if needed (e.g., same name, multiple services).\n",
        "\n",
        "It is important that You ask follow-up questions to gather missing fields based on this schema:\n",
        "- `customer_table` needs customer_id.\n",
        "- `customer_service_table` needs customer_id + service_id.\n",
        "- `billing_table` needs customer_id + service_id (+ optional billing_date, status, plan_name).\n",
        "\n",
        "When ready, construct a SQL-like query or structured representation to fetch the data.\"\"\"\n",
        "prompt_template.messages[0].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c9e4fe",
      "metadata": {
        "id": "52c9e4fe"
      },
      "outputs": [],
      "source": [
        "system_message = prompt_template.format(dialect=\"SQLite\", top_k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a7f2f2",
      "metadata": {
        "id": "c7a7f2f2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "sql_agent_executor = create_react_agent(model= llm,\n",
        "                                        tools= tools,\n",
        "                                        name= 'sql_agent_executor',\n",
        "                                        prompt= system_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc31cd52",
      "metadata": {
        "id": "bc31cd52"
      },
      "outputs": [],
      "source": [
        "# question = \"How much did I spend in the last 6 months?\"\n",
        "\n",
        "# for step in sql_agent_executor.stream(\n",
        "#     {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n",
        "#     stream_mode=\"values\",\n",
        "# ):\n",
        "#     step[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ff724b",
      "metadata": {
        "id": "42ff724b"
      },
      "source": [
        "### Create RAG Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7133d6",
      "metadata": {
        "id": "7e7133d6"
      },
      "outputs": [],
      "source": [
        "from langchain.tools.retriever import create_retriever_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8000e66",
      "metadata": {
        "id": "a8000e66"
      },
      "outputs": [],
      "source": [
        "# from DataProcessing.DocumentParser import DocumentParser\n",
        "# from IndexGeneration.DocumentChunk import DocumentChunk\n",
        "\n",
        "# parser = DocumentParser()\n",
        "# chunker = DocumentChunk()\n",
        "\n",
        "# dataframe = pd.DataFrame()\n",
        "# for pdf in ['HOW5220_DG558_XB6_getting_started.pdf','RSA_01012025.pdf']:\n",
        "#     result = parser.extract_text(pdf,type='local')\n",
        "#     chunk_df = chunker.process_chunk(pd.DataFrame(result),1500)\n",
        "#     chunk_df['source'] = pdf\n",
        "#     dataframe = pd.concat([chunk_df,dataframe])\n",
        "\n",
        "# dataframe = dataframe.reset_index(drop=True)\n",
        "\n",
        "# documents = []\n",
        "# for _,row in dataframe.iterrows():\n",
        "#     documents.append(\n",
        "#         Document(page_content=row['merged_content'],\n",
        "#                  metadata={\n",
        "#                      'source':row['source'],\n",
        "#                      'page_range':row['page_range'],\n",
        "#                      'chunk_coordinates':row['chunk_coordinates']\n",
        "#                  })\n",
        "#     )\n",
        "\n",
        "# index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
        "# vector_store = FAISS(\n",
        "#     embedding_function=embeddings,\n",
        "#     index=index,\n",
        "#     docstore=InMemoryDocstore(),\n",
        "#     index_to_docstore_id={},\n",
        "# )\n",
        "# uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "# vector_store.add_documents(documents=documents, ids=uuids)\n",
        "# vector_store.save_local('vector_store_index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe49e46c",
      "metadata": {
        "id": "fe49e46c"
      },
      "outputs": [],
      "source": [
        "vector_store = FAISS.load_local(\n",
        "    \"vector_store_index\", embeddings, allow_dangerous_deserialization=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "bff8189b",
      "metadata": {
        "id": "bff8189b",
        "outputId": "83ccfdc6-7f23-45e5-8265-aa1c761a1fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "source": [
        "# from DataProcessing.DocumentParser import DocumentParser\n",
        "# from IndexGeneration.DocumentChunk import DocumentChunk\n",
        "\n",
        "# parser = DocumentParser()\n",
        "# chunker = DocumentChunk()\n",
        "\n",
        "# dataframe = pd.DataFrame()\n",
        "# for pdf in ['HOW5220_DG558_XB6_getting_started.pdf','RSA_01012025.pdf']:\n",
        "#     result = parser.extract_text(pdf,type='local')\n",
        "#     chunk_df = chunker.process_chunk(pd.DataFrame(result),1500)\n",
        "#     chunk_df['source'] = pdf\n",
        "#     dataframe = pd.concat([chunk_df,dataframe])\n",
        "\n",
        "# dataframe = dataframe.reset_index(drop=True)\n",
        "\n",
        "# documents = []\n",
        "# for _,row in dataframe.iterrows():\n",
        "#     documents.append(\n",
        "#         Document(page_content=row['merged_content'],\n",
        "#                  metadata={\n",
        "#                      'source':row['source'],\n",
        "#                      'page_range':row['page_range'],\n",
        "#                      'chunk_coordinates':row['chunk_coordinates']\n",
        "#                  })\n",
        "#     )\n",
        "\n",
        "# index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
        "# vector_store = FAISS(\n",
        "#     embedding_function=embeddings,\n",
        "#     index=index,\n",
        "#     docstore=InMemoryDocstore(),\n",
        "#     index_to_docstore_id={},\n",
        "# )\n",
        "# uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "# vector_store.add_documents(documents=documents, ids=uuids)\n",
        "# vector_store.save_local('vector_store_index')\n",
        "\n",
        "# The above code is commented out, preventing the definition of `vector_store`.\n",
        "# Uncomment the following lines to load the existing vector store:\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import AzureOpenAIEmbeddings # Make sure to import AzureOpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "                            api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "                            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
        "                            azure_deployment=os.getenv('AZURE_OPENAI_EMBEDDINGS_MODEL'),\n",
        "                            openai_api_version=os.getenv('AZURE_OPENAI_VERSION'),\n",
        "                            chunk_size = 1\n",
        "                            )\n",
        "\n",
        "vector_store = FAISS.load_local(\n",
        "    \"vector_store_index\", embeddings, allow_dangerous_deserialization=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "a6d47f2c",
      "metadata": {
        "id": "a6d47f2c"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_message = ChatPromptTemplate([\n",
        "    (\"system\", \"You are helpful assitant. You need to help user to answer query related to company's policies guidelines\"),\n",
        "     \"{messages}\"\n",
        "])"
      ]
    },
    {
      "source": [
        "!pip install --upgrade langgraph langchain\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Assuming 'vector_store' is your vector store instance\n",
        "retriever_tool = create_retriever_tool(\n",
        "    vector_store,\n",
        "    name=\"search_company_docs\",\n",
        "    description=\"Useful for when you need to answer questions about company's policies and guidelines. Input should be a search query.\"\n",
        ")\n",
        "\n",
        "# Pass the 'llm' to the 'model' argument instead of 'llm_chain'\n",
        "rag_react_agent = create_react_agent(\n",
        "    model=llm,  # Changed from 'llm_chain' to 'model'\n",
        "    tools=[retriever_tool],\n",
        "    name='rag_agent_executor',\n",
        "    prompt=system_message\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tw6pb14Yv7-g"
      },
      "id": "tw6pb14Yv7-g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NcJyZzAgqbbM",
        "outputId": "7d232b12-a854-47a3-a90c-861e49cf44fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionDeniedError",
          "evalue": "<html>\n  <head>\n    <style global>body{font-family:Arial,Helvetica,sans-serif}.container{align-items:center;display:flex;flex-direction:column;gap:2rem;height:100%;justify-content:center;width:100%}@keyframes enlarge-appear{0%{opacity:0;transform:scale(75%) rotate(-90deg)}to{opacity:1;transform:scale(100%) rotate(0deg)}}.logo{color:#8e8ea0}.scale-appear{animation:enlarge-appear .4s ease-out}@media (min-width:768px){.scale-appear{height:48px;width:48px}}.data:empty{display:none}.data{border-radius:5px;color:#8e8ea0;text-align:center}@media (prefers-color-scheme:dark){body{background-color:#343541}.logo{color:#acacbe}}</style>\n  <meta http-equiv=\"refresh\" content=\"360\">\n</head>\n  <body>\n    <div class=\"container\">\n      <div class=\"logo\">\n        <svg\n          width=\"41\"\n          height=\"41\"\n          viewBox=\"0 0 41 41\"\n          fill=\"none\"\n          xmlns=\"http://www.w3.org/2000/svg\"\n          strokeWidth=\"2\"\n          class=\"scale-appear\"\n        >\n          <path\n            d=\"M37.5324 16.8707C37.9808 15.5241 38.1363 14.0974 37.9886 12.6859C37.8409 11.2744 37.3934 9.91076 36.676 8.68622C35.6126 6.83404 33.9882 5.3676 32.0373 4.4985C30.0864 3.62941 27.9098 3.40259 25.8215 3.85078C24.8796 2.7893 23.7219 1.94125 22.4257 1.36341C21.1295 0.785575 19.7249 0.491269 18.3058 0.500197C16.1708 0.495044 14.0893 1.16803 12.3614 2.42214C10.6335 3.67624 9.34853 5.44666 8.6917 7.47815C7.30085 7.76286 5.98686 8.3414 4.8377 9.17505C3.68854 10.0087 2.73073 11.0782 2.02839 12.312C0.956464 14.1591 0.498905 16.2988 0.721698 18.4228C0.944492 20.5467 1.83612 22.5449 3.268 24.1293C2.81966 25.4759 2.66413 26.9026 2.81182 28.3141C2.95951 29.7256 3.40701 31.0892 4.12437 32.3138C5.18791 34.1659 6.8123 35.6322 8.76321 36.5013C10.7141 37.3704 12.8907 37.5973 14.9789 37.1492C15.9208 38.2107 17.0786 39.0587 18.3747 39.6366C19.6709 40.2144 21.0755 40.5087 22.4946 40.4998C24.6307 40.5054 26.7133 39.8321 28.4418 38.5772C30.1704 37.3223 31.4556 35.5506 32.1119 33.5179C33.5027 33.2332 34.8167 32.6547 35.9659 31.821C37.115 30.9874 38.0728 29.9178 38.7752 28.684C39.8458 26.8371 40.3023 24.6979 40.0789 22.5748C39.8556 20.4517 38.9639 18.4544 37.5324 16.8707ZM22.4978 37.8849C20.7443 37.8874 19.0459 37.2733 17.6994 36.1501C17.7601 36.117 17.8666 36.0586 17.936 36.0161L25.9004 31.4156C26.1003 31.3019 26.2663 31.137 26.3813 30.9378C26.4964 30.7386 26.5563 30.5124 26.5549 30.2825V19.0542L29.9213 20.998C29.9389 21.0068 29.9541 21.0198 29.9656 21.0359C29.977 21.052 29.9842 21.0707 29.9867 21.0902V30.3889C29.9842 32.375 29.1946 34.2791 27.7909 35.6841C26.3872 37.0892 24.4838 37.8806 22.4978 37.8849ZM6.39227 31.0064C5.51397 29.4888 5.19742 27.7107 5.49804 25.9832C5.55718 26.0187 5.66048 26.0818 5.73461 26.1244L13.699 30.7248C13.8975 30.8408 14.1233 30.902 14.3532 30.902C14.583 30.902 14.8088 30.8408 15.0073 30.7248L24.731 25.1103V28.9979C24.7321 29.0177 24.7283 29.0376 24.7199 29.0556C24.7115 29.0736 24.6988 29.0893 24.6829 29.1012L16.6317 33.7497C14.9096 34.7416 12.8643 35.0097 10.9447 34.4954C9.02506 33.9811 7.38785 32.7263 6.39227 31.0064ZM4.29707 13.6194C5.17156 12.0998 6.55279 10.9364 8.19885 10.3327C8.19885 10.4013 8.19491 10.5228 8.19491 10.6071V19.808C8.19351 20.0378 8.25334 20.2638 8.36823 20.4629C8.48312 20.6619 8.64893 20.8267 8.84863 20.9404L18.5723 26.5542L15.206 28.4979C15.1894 28.5089 15.1703 28.5155 15.1505 28.5173C15.1307 28.5191 15.1107 28.516 15.0924 28.5082L7.04046 23.8557C5.32135 22.8601 4.06716 21.2235 3.55289 19.3046C3.03862 17.3858 3.30624 15.3413 4.29707 13.6194ZM31.955 20.0556L22.2312 14.4411L25.5976 12.4981C25.6142 12.4872 25.6333 12.4805 25.6531 12.4787C25.6729 12.4769 25.6928 12.4801 25.7111 12.4879L33.7631 17.1364C34.9967 17.849 36.0017 18.8982 36.6606 20.1613C37.3194 21.4244 37.6047 22.849 37.4832 24.2684C37.3617 25.6878 36.8382 27.0432 35.9743 28.1759C35.1103 29.3086 33.9415 30.1717 32.6047 30.6641C32.6047 30.5947 32.6047 30.4733 32.6047 30.3889V21.188C32.6066 20.9586 32.5474 20.7328 32.4332 20.5338C32.319 20.3348 32.154 20.1698 31.955 20.0556ZM35.3055 15.0128C35.2464 14.9765 35.1431 14.9142 35.069 14.8717L27.1045 10.2712C26.906 10.1554 26.6803 10.0943 26.4504 10.0943C26.2206 10.0943 25.9948 10.1554 25.7963 10.2712L16.0726 15.8858V11.9982C16.0715 11.9783 16.0753 11.9585 16.0837 11.9405C16.0921 11.9225 16.1048 11.9068 16.1207 11.8949L24.1719 7.25025C25.4053 6.53903 26.8158 6.19376 28.2383 6.25482C29.6608 6.31589 31.0364 6.78077 32.2044 7.59508C33.3723 8.40939 34.2842 9.53945 34.8334 10.8531C35.3826 12.1667 35.5464 13.6095 35.3055 15.0128ZM14.2424 21.9419L10.8752 19.9981C10.8576 19.9893 10.8423 19.9763 10.8309 19.9602C10.8195 19.9441 10.8122 19.9254 10.8098 19.9058V10.6071C10.8107 9.18295 11.2173 7.78848 11.9819 6.58696C12.7466 5.38544 13.8377 4.42659 15.1275 3.82264C16.4173 3.21869 17.8524 2.99464 19.2649 3.1767C20.6775 3.35876 22.0089 3.93941 23.1034 4.85067C23.0427 4.88379 22.937 4.94215 22.8668 4.98473L14.9024 9.58517C14.7025 9.69878 14.5366 9.86356 14.4215 10.0626C14.3065 10.2616 14.2466 10.4877 14.2479 10.7175L14.2424 21.9419ZM16.071 17.9991L20.4018 15.4978L24.7325 17.9975V22.9985L20.4018 25.4983L16.071 22.9985V17.9991Z\"\n            fill=\"currentColor\"\n          />\n        </svg>\n      </div>\n      <div class=\"data\"><div class=\"main-wrapper\" role=\"main\"><div class=\"main-content\"><noscript><div class=\"h2\"><span id=\"challenge-error-text\">Enable JavaScript and cookies to continue</span></div></noscript></div></div><script>(function(){window._cf_chl_opt={cvId: '3',cZone: \"platform.openai.com\",cType: 'managed',cRay: '932d90fd1f1a1d62',cH: 'GHMJA_LxEYtJhdjdWy4Gwu68OWHMTYFBCpvuesUJNzY-1745077918-1.2.1.1-yjSYpbiAayMgWG_jhj6Ix0D276eSmyQjVbT_347Mb4x6Yu0STmBp9jE1_.bTydAI',cUPMDTk: \"\\/api-keys\\/openai\\/deployments\\/gpt-4o\\/chat\\/completions?api-version=2023-05-15&__cf_chl_tk=mGIompd8YYcKJTsLugDmGnv2w1saOL1ua95jL51flYI-1745077918-1.0.1.1-sCqbwu42KyNfKutrwuoZxGC7LP561qItCupx3YemZ6A\",cFPWv: 'g',cITimeS: '1745077918',cTplC: 1,cTplV: 5,cTplB: 'cf',cK: \"unsupported_browser_beacon\",fa: \"\\/api-keys\\/openai\\/deployments\\/gpt-4o\\/chat\\/completions?api-version=2023-05-15&__cf_chl_f_tk=mGIompd8YYcKJTsLugDmGnv2w1saOL1ua95jL51flYI-1745077918-1.0.1.1-sCqbwu42KyNfKutrwuoZxGC7LP561qItCupx3YemZ6A\",md: \"vqcex5Te0RxL3RaJWmx_BMHQhQQ_giKtQRxC5IDIOmU-1745077918-1.2.1.1-gEsa7uku1gUTflTPe2C8TEPcv484czKaJ0jt7d2Tpm5bzpwQYjR7XBbbjtApagbpVPlznPrB2s_8WPm8APtBTvdui49jsUw2F.9T12Kr8banPa5NreHefRLwm1txtBVmeoomp75sIkZ4k52_IJHwpNyPbyXfEttBiVLeHVGurzIjOhC2wPokP.TCKMqVewTW256MczNH1eOUNO4zCoaFMw7vBdCGfThT6VENfLt1.VnVnZjCPTL6.Vq6X7j8DJhEVvSi80X_0kIzmaeUxiA5k9UXrMcmurinD56VknmC6jYdDc1ChS0W_ASXrHI2AjGXEqAuimrvd0mDSs4efnl0i23X7EKXHf5efkFMdzc.MWUM6Fvo_Hl86XTa.AjG41JfhDvPf4l0ZJBRf_Q.hxyv.SNe.1WLuCXuzw.b0dhLRly0p31UFzs5DztxTbZwP0dAYArZDzpdMYGL2ATWj2Jmb3CYsGA9cqkvFJfXEhw9LwosGBUOu9aMxDf8Umhq9WPyOkaN071hWBPf4TNVeCfmLcktbiir15UDz5ejUM2tugQ0MkhJVXNv_EcdUYQipaSWAWLiBM2eaJOqRCbE5_DsyTk9qNWk.gwrdaB8aCGjT3E8a62yP9Hq4J8tbsVcjhnpt4dbr4GUhmzo9t0tbG91FyFjAIEQLwbhFIdxxMls0M9BHQJVIzNaUUI4fcnuJhVXhFL5DAgDALBTvsJsr5tJdAW6uc5zAjn84dgOXXMvUvR62W2JQG3C9_e4RJ8ghqFqgOOqgQ1ggqOlBsheOj.a3Pf_Ht9ljqY1uVN10Ma5HcCkYc0jWpY.BBlXhNJjCA7P6ya_jwrOuU_qokcnjY9B_xNf8Fl29cQrEbEZklEvnSVkl5lW0.tPR4viLjxHKawnkyl7y89gFk36xDUJ7I4ywulrCLNthuK6E0aF3eXW6IxjJ51Sw76UFOe5H4akrgoAe6Dh4tHWkGpKwnP4Kga_p3NT9BV3yX17NMs0PO2t4KlaF4uI2dNg9EkiqPbBOPq6cejVoUGLp1J1R6S2.P45bs_9f3KmM4K8PwUd6hmR57e4IevQ2UIdgvEOdYs_frxGRQ.D6T0X2bnM6.4O2aanl3IMTJ3vANfzBfJ.8Y22FucSSQhlVFO1T6iSPHbvpIJXKUlCFnqRqTvFc9cmLeAQxJbzqZVHHLDr.Yea4ig9hJ8\",mdrd: \"Up8KkxOCgjwvGtkNyxcmxnwokZLGnTxzLLhsLV42uDQ-1745077918-1.2.1.1-VEMfW8k8CzI7AYtiJ1GlW3nGKI8JusIv7HWu_b4SAR6P2DT6iHpqQTE_r7Ktbaxc2sDtufsxrQ.1X_Gyhok3oMxeuWChbdW.BI77CzlWYef_6KE0iuJHgXYL5fo9.7pN.uDgrtr6KmaCj8oDI5pwEfF2Gjh2zCkCimW9SRhldNf1cenU53ncZXtlou9rocsMrrcCDa_EYWl_SmOr0fR4iMEvzqP9gfJrcu8O9NQqNm_CzskaSvMgBR1AXJ5pjCuz.cP5Adqkhh1v7lxKmIN8hQ06mE5su7YYQu0t_H27UAFo9M.0m9pwETLSrULiuEuEShV9Q22mdlIYyfM_AcJyOwazysv1xoxF6v0TDcaKVlgbPmPGmpThwHWF535cnz3ljSsmuCWhY4dmndxyw3ST5I8avTqMswBgLlIEworoRJVK3pah1hB_xLKtXNm.MuW1UBUZR1chLj7RscNwuAr_1OwgdzWmLBxUEK5ig5u_WHZHWSG2auXsvCVGfRtuep9PQNePXVa_lcVGvyVFKK9l30MZdxCCKCXx0DLbAA_QPCRD4yDq5H4ysDC4EpvKjb1.nfRVipxNWGw82VppuWcJp84QUU2WM5Dla.Tgm0s08OVGS6idLlnik01OeNhKockrZCkFNr6f_yZZe7aTejPOfM94sESLupOjC_lfhrUW34dKHVv8qTRcEcZEzBc_egXvr0fbQhRDpPiMdILTJyFy5U0nr7fP__3UcCZYBPSMo4oLjafq5G3UuUL2DriXPyunRRELiowf91hgg1mCbFk2inkijmi4H91KUWr7p0i20PCVe5yUG0CpeFW15zRB3E9ucdkMNQHnAe165vExZOGus2MUnvEuwMyw51DdrbUpA55pWD86N1dLlZ2gi5IIq0onHTx7T96x0ce2GU5DIAYgEE8Ag3AuEj9ya2RYv8St4tXmtZEO1Qp_96FPenmjS_wk0kAFB3SuTaeIL5iUy22ntdtt68gez7PzrHv3XnjIj_njQH_No1_zvEKX.nNcpJ4slrScBVi1R.f3tNSUX.KKGy9X9_q2D6i74wP7.5NvC_PykRqUHUyUMOnu4fp.iL3Rs.13C6Rj9e6Xt2ENl_jxFpIaG6h3oLTtN8eilplXQJWYIQLHVHVkaLdT1Qzj3nuI0HPyzJIZFSUMyQdfF2lzxO.Qj1TMoyzBqi5nxhVhFaVO9swV1vb4LPQXY2utbcS8_suVmcFu.MQbpVtf.tgaRCDfC0wCA9rfZtapDL8zib5GXLc1u33UAW0i0arWAGNysU0S2HmRqQ1VEBV4moTC85GTHs2hoeu68ZphFAJBanW5Ox9m5EE5VV4RP.1YO7n458nggpuVWC4iRgcxwKdrqrFchzdjlJViWJXO.tYYjcPVwtKcuzregZE.E.Z7sOKlPWJiiaLxIOrciwq.hHWtjMMXk7jlTbiuQTAWI6KK3.BDKgMV338ojWfoxkS.8K2F0nhGwn_FkGF3KAonXZ90_vRdSYwo0LGJGNzGkVQgpikvlLx4Bpmp8blGREDeOQ2Ow_JIgb0eyxx3rjL.Cf9GPC85O6SfLgxb9IkcvHWc3VG0kCnlmiAJOZn3jTtfJ92dKdQ6_adICSTUpsNTmcKY2iuuHlTYdnKR3GR7jLfzLgKlY0YJT7192Aix7XbRnaEU9bbNrKJXEboa3n7_.e1A49v9sgNbF6mpCQ3dL7PG2Lmrhr8RflEoPCnImOE_G8EBh25z46Kk8YntbF2UTi__p3dFFcBtVWPa.N.p682_uCAn3z4THra0Di816X1e2iOPOT2S2UR2f6Unij47W7Artk1tVpIOBMiYL6LonHmobemBU7JAiqpDfR4cR8MsQddJpwMDaIvJJ3oHYX9fmbwsp2xt.Kj1phx9TlU2iEb4SFa2LgtxsmKgVONyCdErat4BdDOrHtto9p.H8cVV_tChlQhGmK4pNhTRoKUXqy2lV5oSZ.85HzLvxnpjTW7fpMbgsmIHouvkxW9XsM8QrI_o5XPoOX09woPqmRMwX_zrqG5wZ87Jq.QIDS9z_uMWvLM6vaO1PoZuZKZs.5IPsX19mRy4Ly8KaWWXnqPh8UkvkvkSh0mv_N5cO3JtM.uS2zCtu5Kkifk0krpE5uH7jnWLwtGLJHe9rBHZy0ML_GNTzcqQWrBxrbQRaE8qFaaJd95mihpSZXOmtiy26pBQfoen0dAUFBKlkNnBlOEO.a32Xrpm48RzZIRblCs1s0bR3UjKeYLggoq54FQraxSB9YcUMghdo0Ww.4xz2U6XcQbsQ5ZNCm8g9efcXQxGYGF.RGsbF4AXomR7NmBdUmCavY8Onc.qlD3fvhl9MehsY4i.N0GpmcZxob7HfTktPzLUHMciVBEIPQ_GAsnr1p2wMI2FFkpU9HHoojoHMQqHtOC9fVsUdK5WuQM74STHvuMVZKjRgm4D02lN3kE6jEUN8PY6djH8_on_N0eRLXDUjfOFBiO4i5hY21CZsjQAYSav5vwjSshXfqX1s9nltoCxTf1OtCGH18_nzMQTskvGKt4YZ5xVOZr1wj9._vFeJqDXMa9q0drACN8vpEq8lrLTSFewoqWcuYYVjgp.bjL_SkfWRO_KoPOMKyxbt9lL4ZoQBQfv6lDrOYgSJiWXI9R3U2edN6dFXYeuWcVMxJs7Rs0uXPPC5V4nzcDi6idtKOgdnwNlWa3cOMb7bkB6YXs8DQDJTHddQXQfpkxqezWhag8uhNaOGJea8R7vgedpSBohyyDA1RYHPVn3ult2SznJJFs_6dAMEpr8dL4J5q8XtVz.KlKqoBV_Vs14swn3xzQ0ZXPqQ2oANPOeBbfhA06MEUxE8jVc7EyL7klRzZnKuOpPguqxw3j.QY2HEZbeQeC6iIMhvTyHdB4ix14A2PtesflHYiUBVkMiIg8d3K9zbyjXtBwmA1WhEgiBvsfI1h4aLMaCgrZmX_eEIFA.LxNpLeocAmaFkhbhITxspTC6Sv.SbwsK8yGHoGCuXJsY2d9VHS8aOFIcptjFu_bKSfN52m7kQooOirbZPNDqOBRZassR_gHn5VnwUtiM0VWHb3AaTpQlnpNu0wGvdB_aUEQqbLEguRRDgTY2CXEizjrIpP310jEDMPrEqbH9OHJlQxOPwG_roE9G5ztiDio0N53nAW9275o0P2o9NhO0KS3CyP4bdm1TTtaM1WsiEcyANxynLmAxNBYGXdw35WnH9fNzMbKTvVuZn6ITH5vh9Rh0reEYd5pBl56.Kpeajo4lWqkGHaQUvTtXHtgfNtxggaKjzEKFGO8bPrxJTpBt7iWBk6F1nZ.loWwk_GqYbINho8a5sA07wey5jmj9RrjtHPxSTWKqyH07unTjKYPh70Slmos6UDjb70xW9kSvUUSWkDFNxsRa128BSDg1D7irOllQRQf4LBrJ7OU.lzLboteFtbbEGCArzosxUhnuFUV.hGU0R9ukYLEXsXzZPlTKXqs3EOBP0WBYPVpA5P.L780S_T6h5uNDQaIsFfHrh75220c8Zcy1_1_cMvGZXubVxqj8SKIBQ66O5geM8oj_VEvwIGxr1SyLl19VhG1jmh9B01jWG8aFmZDV9uFA2a7XGIpQsoY0QA\"};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=932d90fd1f1a1d62';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/api-keys\\/openai\\/deployments\\/gpt-4o\\/chat\\/completions?api-version=2023-05-15&__cf_chl_rt_tk=mGIompd8YYcKJTsLugDmGnv2w1saOL1ua95jL51flYI-1745077918-1.0.1.1-sCqbwu42KyNfKutrwuoZxGC7LP561qItCupx3YemZ6A\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());</script></div>\n    </div>\n  <script>!function(){var e=document.createElement(\"iframe\");function n(){var n=e.contentDocument||e.contentWindow.document;if(n){var t=n.createElement(\"script\");t.nonce=\"\",t.innerHTML=\"window['__CF$cv$params']={r:'792f8224776acf9f',m:'hMcSCCrnIkr7c8Pec6Na6boaaFAnQ6S0ypG2GKRbKgc-1675305063-0-AaJn0SqKZQnadmRQ5O1dM9xMkXWyP+ll7gpl2NHeoNbZTEXMjlB10KkwnEU3hf0/gMODfKqcBGLVecql6U04GGs+iJ/kNrNqj1FgfAOlQV+T2koMQMvUy1zr9tegBBX6BikfccHZhwoJhnXc0eTcg58=',s:[0x60b082f691,0xee65a67e11],u:'/cdn-cgi/challenge-platform/h/b'};var now=Date.now()/1000,offset=14400,ts=''+(Math.floor(now)-Math.floor(now%offset)),_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/h/b/scripts/alpha/invisible.js?ts='+ts,document.getElementsByTagName('head')[0].appendChild(_cpo);\",n.getElementsByTagName(\"head\")[0].appendChild(t)}}if(e.height=1,e.width=1,e.style.position=\"absolute\",e.style.top=0,e.style.left=0,e.style.border=\"none\",e.style.visibility=\"hidden\",document.body.appendChild(e),\"loading\"!==document.readyState)n();else if(window.addEventListener)document.addEventListener(\"DOMContentLoaded\",n);else{var t=document.onreadystatechange||function(){};document.onreadystatechange=function(e){t(e),\"loading\"!==document.readyState&&(document.onreadystatechange=t,n())}}}();</script></body>\n</html>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-ea35d75c2cdd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'what is liability for the damage?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrag_react_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2737\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2738\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2739\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   2740\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2375\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2376\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2377\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2378\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2379\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/prebuilt/chat_agent_executor.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStateSchema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStateSchema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_input_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAIMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_runnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0;31m# add agent name to the AIMessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3033\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3034\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3035\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3036\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5414\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5415\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5416\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5417\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5418\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m         return cast(\n\u001b[1;32m    367\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    369\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    936\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m                 results.append(\n\u001b[0;32m--> 759\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    760\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1003\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    928\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         )\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    950\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mPermissionDeniedError\u001b[0m: <html>\n  <head>\n    <style global>body{font-family:Arial,Helvetica,sans-serif}.container{align-items:center;display:flex;flex-direction:column;gap:2rem;height:100%;justify-content:center;width:100%}@keyframes enlarge-appear{0%{opacity:0;transform:scale(75%) rotate(-90deg)}to{opacity:1;transform:scale(100%) rotate(0deg)}}.logo{color:#8e8ea0}.scale-appear{animation:enlarge-appear .4s ease-out}@media (min-width:768px){.scale-appear{height:48px;width:48px}}.data:empty{display:none}.data{border-radius:5px;color:#8e8ea0;text-align:center}@media (prefers-color-scheme:dark){body{background-color:#343541}.logo{color:#acacbe}}</style>\n  <meta http-equiv=\"refresh\" content=\"360\">\n</head>\n  <body>\n    <div class=\"container\">\n      <div class=\"logo\">\n        <svg\n          width=\"41\"\n          height=\"41\"\n          viewBox=\"0 0 41 41\"\n          fill=\"none\"\n          xmlns=\"http://www.w3.org/2000/svg\"\n          strokeWidth=\"2\"\n          class=\"scale-appear\"\n        >\n          <path\n            d=\"M37.5324 16.8707C37.9808 15.5241 38.1363 14.0974 37.9886 12.6859C37.8409 11.2744 37.3934 9.91076 36.676 8.68622C35.6126 6.83404 33.9882 5.3676 32.0373 4.4985C30.0864 3.62941 27.9098 3.40259 25.8215 3.85078C24.8796 2.7893 23.7219 1.94125 22.4257 1.36341C21.1295 0.785575 19.7249 0.491269 18.3058 0.500197C16.1708 0.495044 14.0893 1.16803 12.3614 2.42214C10.6335 3.67624 9.34853 5.44666 8.6917 7.47815C7.30085 7.76286 5.98686 8.3414 4.8377 9.17505C3.68854 10.0087 2.73073 11.0782 2.02839 12.312C0.956464 14.1591 0.498905 16.2988 0.721698 18.4228C0.944492 20.5467 1.83612 22.5449 3.268 24.1293C2.81966 25.4759 2.66413 26.9026 2.81182 28.3141C2.95951 29.7256 3.40701 31.0892 4.12437 32.3138C5.18791 34.1659 6.8123 35.6322 8.76321 36.5013C10.7141 37.3704 12.8907 37.5973 14.9789 37.1492C15.9208 38.2107 17.0786 39.0587 18.3747 39.6366C19.6709 40.2144 21.0755 40.5087 22.4946 40.4998C24.6307 40.5054 26.7133 39.8321 28.4418 38.5772C30.1704 37.3223 31.4556 35.5506 32.1119 33.5179C33.5027 33.2332 34.8167 32.6547 35.9659 31.821C37.115 30.9874 38.0728 29.9178 38.7752 28.684C39.8458 26.8371 40.3023 24.6979 40.0789 22.5748C39.8556 20.4517 38.9639 18.4544 37.5324 16.8707ZM22.4978 37.8849C20.7443 37.8874 19.0459 37.2733 17.6994 36.1501C17.7601 36.117 17.8666 36.0586 17.936 36.0161L25.9004 31.4156C26.1003 31.3019 26.2663 31.137 26.3813 30.9378C26.4964 30.7386 26.5563 30.5124 26.5549 30.2825V19.0542L29.9213 20.998C29.9389 21.0068 29.9541 21.0198 29.9656 21.0359C29.977 21.052 29.9842 21.0...\n            fill=\"currentColor\"\n          />\n        </svg>\n      </div>\n      <div class=\"data\"><div class=\"main-wrapper\" role=\"main\"><div class=\"main-content\"><noscript><div class=\"h2\"><span id=\"challenge-error-text\">Enable JavaScript and cookies to continue</span></div></noscript></div></div><script>(function(){window._cf_chl_opt={cvId: '3',cZone: \"platform.openai.com\",cType: 'managed',cRay: '932d90fd1f1a1d62',cH: 'GHMJA_LxEYtJhdjdWy4Gwu68OWHMTYFBCpvuesUJNzY-1745077918-1.2.1.1-yjSYpbiAayMgWG_jhj6Ix0D276eSmyQjVbT_347Mb4x6Yu0STmBp9jE1_.bTydAI',cUPMDTk: \"\\/api-keys\\/openai\\/deployments\\/gpt-4o\\/chat\\/completions?api-version=2023-05-15&__cf_chl_tk=mGIompd8YYcKJTsLugDmGnv2w1saOL1ua95jL51flYI-1745077918-1.0.1.1-sCqbwu42KyNfKutrwuoZxGC7LP561qItCupx3YemZ6A\",cFPWv: 'g',cITimeS: '1745077918',cTplC: 1,cTplV: 5,cTplB: 'cf',cK: \"unsupported_browser_beacon\",fa: \"\\/api-keys\\/openai\\/deployments\\/gpt-4o\\/chat\\/completions?api-version=2023-05-15&__cf_chl_f_tk=mGIompd8YYcKJTsLugDmGnv2w1saOL1ua95jL51flYI-1745077918-1.0.1.1-sCqbwu42KyNfKutrwuoZxGC7LP561qItCupx3YemZ6A\",md: \"vqcex5Te0RxL3RaJWmx_BMHQhQQ_giKtQRxC5IDIOmU-1745077918-1.2.1.1-gEsa7uku1gUTflTPe2C8TEPcv484czKaJ0jt7d2Tpm5bzpwQYjR7XBbbjtApagbpVPlznPrB2s_8WPm8APtBTvdui49jsUw2F.9T12Kr8banPa5NreHefRLwm1txtBVmeoomp75sIkZ4k52_IJHwpNyPbyXfEttBiVLeHVGurzIjOhC2wPokP.TCKMqVewTW256MczNH1eOUNO4zCoaFMw7vBdCGfThT6VENfLt1.VnVnZjCPTL6.Vq6X7j8DJhEVvSi80X_0kIzmaeUxiA5k9UXrMcmurinD56VknmC6jYdDc1ChS0W_ASXrHI2AjGXEqAuimrvd0mDSs4efnl0i23X7EKXHf5efkFMdzc.MWUM6Fvo_Hl86XTa.AjG41JfhDvPf4l0ZJBRf_Q.hxyv.SNe.1WLuCXuzw.b0dhLRly0p31UFz...\n    </div>\n  <script>!function(){var e=document.createElement(\"iframe\");function n(){var n=e.contentDocument||e.contentWindow.document;if(n){var t=n.createElement(\"script\");t.nonce=\"\",t.innerHTML=\"window['__CF$cv$params']={r:'792f8224776acf9f',m:'hMcSCCrnIkr7c8Pec6Na6boaaFAnQ6S0ypG2GKRbKgc-1675305063-0-AaJn0SqKZQnadmRQ5O1dM9xMkXWyP+ll7gpl2NHeoNbZTEXMjlB10KkwnEU3hf0/gMODfKqcBGLVecql6U04GGs+iJ/kNrNqj1FgfAOlQV+T2koMQMvUy1zr9tegBBX6BikfccHZhwoJhnXc0eTcg58=',s:[0x60b082f691,0xee65a67e11],u:'/cdn-cgi/challenge-platform/h/b'};var now=Date.now()/1000,offset=14400,ts=''+(Math.floor(now)-Math.floor(now%offset)),_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/h/b/scripts/alpha/invisible.js?ts='+ts,document.getElementsByTagName('head')[0].appendChild(_cpo);\",n.getElementsByTagName(\"head\")[0].appendChild(t)}}if(e.height=1,e.width=1,e.style.position=\"absolute\",e.style.top=0,e.style.left=0,e.style.border=\"none\",e.style.visibility=\"hidden\",document.body.appendChild(e),\"loading\"!==document.readyState)n();else if(window.addEventListener)document.addEventListener(\"DOMContentLoaded\",n);else{var t=document.onreadystatechange||function(){};document.onreadystatechange=function(e){t(e),\"loading\"!==document.readyState&&(document.onreadystatechange=t,n())}}}();</script></body>\n</html>"
          ]
        }
      ],
      "source": [
        "question = 'what is liability for the damage?'\n",
        "rag_react_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": question}]})"
      ],
      "id": "NcJyZzAgqbbM"
    },
    {
      "source": [
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.agents import Tool, AgentExecutor # Import necessary modules\n",
        "\n",
        "\n",
        "# Load LLM, using AzureChatOpenAI instead of ChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_deployment=os.getenv('AZURE_GPT4O_MODEL'),\n",
        "    temperature=0,\n",
        "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
        "    openai_api_version=os.getenv('AZURE_OPENAI_VERSION'),\n",
        ")\n",
        "\n",
        "# Define retriever - This was missing\n",
        "# Replace with your actual retriever initialization\n",
        "# Assuming you're using the vector_store defined earlier\n",
        "from langchain.chains import RetrievalQA  # Import RetrievalQA\n",
        "\n",
        "retriever = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever())\n",
        "\n",
        "# Define tool\n",
        "retriever_tool = Tool(\n",
        "    name=\"legal_doc_search\",\n",
        "    func=retriever.run,  # Use retriever.run instead of retriever.invoke\n",
        "    description=\"Useful for answering questions about legal document content\"\n",
        ")\n",
        "\n",
        "# Create agent\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[retriever_tool],\n",
        "    name=\"rag_agent\",\n",
        "    prompt=system_message  # Optional, or use default\n",
        ")\n",
        "\n",
        "# Executor\n",
        "agent_executor = AgentExecutor(agent=agent, tools=[retriever_tool], verbose=True)\n",
        "\n",
        "# Ask your question\n",
        "response = agent_executor.invoke({\"input\": \"What is liability for the damage?\"})\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FaMlittmxDGj",
        "outputId": "7bd9c3e7-221d-431b-8b95-83be8a7b1a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "id": "FaMlittmxDGj",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for VectorStoreRetriever\nvectorstore\n  Input should be an instance of VectorStore [type=is_instance_of, input_value=<langchain_community.vect...bject at 0x7c35c3f56ad0>, input_type=FAISS]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-54b967ece398>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrievalQA\u001b[0m  \u001b[0;31m# Import RetrievalQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Define tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36mas_retriever\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"\"\"\n\u001b[1;32m    998\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_retriever_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVectorStoreRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for VectorStoreRetriever\nvectorstore\n  Input should be an instance of VectorStore [type=is_instance_of, input_value=<langchain_community.vect...bject at 0x7c35c3f56ad0>, input_type=FAISS]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of"
          ]
        }
      ]
    },
    {
      "source": [
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.agents import Tool, AgentExecutor # Import necessary modules\n",
        "from langchain.chains import RetrievalQA  # Import RetrievalQA\n",
        "\n",
        "# Load LLM, using AzureChatOpenAI instead of ChatOpenAI\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_deployment=os.getenv('AZURE_GPT4O_MODEL'),\n",
        "    temperature=0,\n",
        "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
        "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
        "    openai_api_version=os.getenv('AZURE_OPENAI_VERSION'),\n",
        ")\n",
        "\n",
        "# Define retriever - This was missing\n",
        "# Replace with your actual retriever initialization\n",
        "# Assuming you're using the vector_store defined earlier\n",
        "# Instead of using vector_store.as_retriever(), pass vector_store directly\n",
        "retriever = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(), vectorstore=vector_store)\n",
        "\n",
        "# Define tool\n",
        "retriever_tool = Tool(\n",
        "    name=\"legal_doc_search\",\n",
        "    func=retriever.run,  # Use retriever.run instead of retriever.invoke\n",
        "    description=\"Useful for answering questions about legal document content\"\n",
        ")\n",
        "\n",
        "# Create agent\n",
        "agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[retriever_tool],\n",
        "    name=\"rag_agent\",\n",
        "    prompt=system_message  # Optional, or use default\n",
        ")\n",
        "\n",
        "# Executor\n",
        "agent_executor = AgentExecutor(agent=agent, tools=[retriever_tool], verbose=True)\n",
        "\n",
        "# Ask your question\n",
        "response = agent_executor.invoke({\"input\": \"What is liability for the damage?\"})\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EOStptJ0xLfa",
        "outputId": "e11585f1-83d8-484b-ca39-30f3026e31ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "id": "EOStptJ0xLfa",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for VectorStoreRetriever\nvectorstore\n  Input should be an instance of VectorStore [type=is_instance_of, input_value=<langchain_community.vect...bject at 0x7c35c3f56ad0>, input_type=FAISS]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-7e458f1373a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Assuming you're using the vector_store defined earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Instead of using vector_store.as_retriever(), pass vector_store directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_chain_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Define tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36mas_retriever\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \"\"\"\n\u001b[1;32m    998\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_retriever_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVectorStoreRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for VectorStoreRetriever\nvectorstore\n  Input should be an instance of VectorStore [type=is_instance_of, input_value=<langchain_community.vect...bject at 0x7c35c3f56ad0>, input_type=FAISS]\n    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd527d61",
      "metadata": {
        "id": "fd527d61"
      },
      "source": [
        "## Supervisor Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6e3953",
      "metadata": {
        "id": "0c6e3953"
      },
      "outputs": [],
      "source": [
        "from langgraph_supervisor import create_supervisor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30864ced",
      "metadata": {
        "id": "30864ced"
      },
      "outputs": [],
      "source": [
        "workflow = create_supervisor(\n",
        "    [sql_agent_executor, rag_react_agent],\n",
        "    model=llm,\n",
        "    prompt=(\n",
        "        \"You are a team supervisor managing a customer billing sql analysis expert and a math expert. \"\n",
        "        \"For customer billing realted questions, use sql_agent_executor. \"\n",
        "        \"For queries related to company's policies and guidelines, use rag_agent_executor.\"\n",
        "        \"Avoid mentioning any agent name in your response\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1587ae64",
      "metadata": {
        "id": "1587ae64"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()\n",
        "result = app.invoke({\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": 'what is liability for the damage?'\n",
        "        }\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "62e8ae65",
      "metadata": {
        "id": "62e8ae65",
        "outputId": "bb8f4227-d8b6-4062-fff5-46a547ea6864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'result' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-0ac921c19f1a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe240288",
      "metadata": {
        "id": "fe240288"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langgraph",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}